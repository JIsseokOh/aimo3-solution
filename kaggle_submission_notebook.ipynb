{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Mathematical Olympiad - Progress Prize 3\n",
    "## SC-TIR (Self-Consistency with Tool-Integrated Reasoning) Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup - only install packages when internet is available (non-submission mode)\nimport os\nIS_SUBMISSION = bool(os.getenv('KAGGLE_IS_COMPETITION_RERUN'))\n\nif not IS_SUBMISSION:\n    # Only install packages during testing (has internet), not during submission\n    import subprocess\n    subprocess.run(['pip', 'install', '-q', 'protobuf==3.20.3'], check=False)\n    subprocess.run(['pip', 'install', '-q', 'sympy'], check=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import traceback\n",
    "from collections import Counter\n",
    "from typing import Optional, List\n",
    "from io import StringIO\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "import polars as pl\n",
    "import torch\n",
    "\n",
    "IS_SUBMISSION = bool(os.getenv('KAGGLE_IS_COMPETITION_RERUN'))\n",
    "print(f\"Is submission: {IS_SUBMISSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration - Qwen2.5-Math-7B on H100 (72B doesn't fit without quantization)\nclass Config:\n    # Using 7B model because:\n    # - 72B needs ~144GB (bfloat16), H100 only has 80GB\n    # - bitsandbytes quantization is NOT available in Kaggle\n    # - 7B model (~14GB) fits easily and is much faster\n    model_id = \"/kaggle/input/qwen2.5-math/transformers/7b-instruct/1\"\n    \n    # More samples since 7B is faster (can afford more for better accuracy)\n    num_samples = 32\n    \n    # Generation parameters\n    temperature = 0.7\n    max_new_tokens = 2048\n    top_p = 0.95\n    \n    # Code execution\n    code_timeout = 10\n    max_code_executions = 3\n    \n    # Enable advanced features\n    enable_feedback_loop = True\n    max_feedback_turns = 2  # More feedback turns since 7B is faster\n    \n    # Self-verification\n    enable_verification = True\n    \n    # No quantization needed for 7B\n    use_4bit = False"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Advanced Chain-of-Thought Prompt\nSYSTEM_PROMPT = \"\"\"You are a world-class mathematician competing in the International Mathematical Olympiad.\n\n## Problem-Solving Strategy\n\n1. **Understand**: Read carefully. Identify what is given and what is asked.\n2. **Plan**: Consider multiple approaches (algebraic, geometric, combinatorial, number-theoretic).\n3. **Execute**: Work through the solution step-by-step with rigorous logic.\n4. **Verify**: Check your answer by substitution or alternative method.\n5. **Compute**: Use Python code for complex calculations.\n\n## Code Execution Rules\n\n- Write Python code inside ```python and ``` tags\n- Use sympy for symbolic math, fractions for exact arithmetic\n- Always print the final numerical result\n- The code will be executed and results provided back to you\n\n## Answer Format\n\n- Your final answer must be a non-negative integer\n- If the problem asks for remainder when divided by 10^5, compute that\n- Put your FINAL answer inside \\\\boxed{} at the very end\n- Double-check before giving the final answer\n\n## Example Solution\n\nProblem: Find the remainder when 2^100 is divided by 7.\n\nLet me solve this step-by-step.\n\nFirst, I'll find the pattern of powers of 2 modulo 7:\n- 2^1 ≡ 2 (mod 7)\n- 2^2 ≡ 4 (mod 7)\n- 2^3 ≡ 8 ≡ 1 (mod 7)\n\nThe pattern repeats with period 3. Since 100 = 33×3 + 1, we have 2^100 ≡ 2^1 ≡ 2 (mod 7).\n\nLet me verify with Python:\n\n```python\nresult = pow(2, 100, 7)\nprint(result)\n```\n\nThe code confirms our answer is 2.\n\nTherefore, the answer is \\\\boxed{2}\n\"\"\"\n\n# Verification prompt for self-checking\nVERIFY_PROMPT = \"\"\"Review your solution and verify the answer is correct.\n\n1. Check all calculations\n2. Verify the logic is sound\n3. If you used code, confirm the code is correct\n4. Make sure the answer format matches what was asked\n\nIf you find an error, provide the corrected answer.\nPut your FINAL verified answer inside \\\\boxed{}\"\"\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_code_blocks(text: str) -> List[str]:\n    \"\"\"Extract Python code blocks from text\"\"\"\n    pattern = r'```python\\s*(.*?)\\s*```'\n    matches = re.findall(pattern, text, re.DOTALL)\n    return matches\n\ndef execute_code_safely(code: str) -> tuple:\n    \"\"\"Execute code safely with extended math libraries\"\"\"\n    import math\n    import cmath\n    import fractions\n    import itertools\n    import functools\n    from decimal import Decimal, getcontext\n    \n    # Set high precision for decimal calculations\n    getcontext().prec = 50\n    \n    try:\n        import sympy\n        from sympy import (\n            Symbol, symbols, solve, simplify, expand, factor,\n            sqrt, Rational, pi, E, I, oo,\n            sin, cos, tan, log, exp, \n            gcd, lcm, mod_inverse, factorial, binomial,\n            isprime, prime, primerange, factorint, divisors,\n            Matrix, det, eye,\n            Sum, Product, limit, diff, integrate,\n            floor, ceiling, Abs\n        )\n    except ImportError:\n        sympy = None\n    \n    try:\n        import numpy as np\n    except ImportError:\n        np = None\n    \n    # Extended safe builtins\n    safe_globals = {\n        \"__builtins__\": {\n            \"abs\": abs, \"all\": all, \"any\": any, \"bin\": bin,\n            \"bool\": bool, \"dict\": dict, \"divmod\": divmod,\n            \"enumerate\": enumerate, \"filter\": filter, \"float\": float,\n            \"format\": format, \"frozenset\": frozenset,\n            \"hash\": hash, \"hex\": hex, \"int\": int, \"isinstance\": isinstance,\n            \"iter\": iter, \"len\": len, \"list\": list, \"map\": map,\n            \"max\": max, \"min\": min, \"next\": next, \"oct\": oct,\n            \"ord\": ord, \"pow\": pow, \"print\": print, \"range\": range,\n            \"repr\": repr, \"reversed\": reversed, \"round\": round,\n            \"set\": set, \"slice\": slice, \"sorted\": sorted,\n            \"str\": str, \"sum\": sum, \"tuple\": tuple, \"type\": type,\n            \"zip\": zip, \"True\": True, \"False\": False, \"None\": None,\n            \"complex\": complex, \"bytes\": bytes, \"bytearray\": bytearray,\n        },\n        \"math\": math,\n        \"cmath\": cmath,\n        \"fractions\": fractions,\n        \"Fraction\": fractions.Fraction,\n        \"itertools\": itertools,\n        \"functools\": functools,\n        \"Decimal\": Decimal,\n        \"reduce\": functools.reduce,\n    }\n    \n    # Add sympy functions if available\n    if sympy:\n        safe_globals[\"sympy\"] = sympy\n        safe_globals[\"Symbol\"] = sympy.Symbol\n        safe_globals[\"symbols\"] = sympy.symbols\n        safe_globals[\"solve\"] = sympy.solve\n        safe_globals[\"simplify\"] = sympy.simplify\n        safe_globals[\"expand\"] = sympy.expand\n        safe_globals[\"factor\"] = sympy.factor\n        safe_globals[\"sqrt\"] = sympy.sqrt\n        safe_globals[\"Rational\"] = sympy.Rational\n        safe_globals[\"pi\"] = sympy.pi\n        safe_globals[\"E\"] = sympy.E\n        safe_globals[\"I\"] = sympy.I\n        safe_globals[\"oo\"] = sympy.oo\n        safe_globals[\"sin\"] = sympy.sin\n        safe_globals[\"cos\"] = sympy.cos\n        safe_globals[\"tan\"] = sympy.tan\n        safe_globals[\"log\"] = sympy.log\n        safe_globals[\"exp\"] = sympy.exp\n        safe_globals[\"gcd\"] = sympy.gcd\n        safe_globals[\"lcm\"] = sympy.lcm\n        safe_globals[\"mod_inverse\"] = sympy.mod_inverse\n        safe_globals[\"factorial\"] = sympy.factorial\n        safe_globals[\"binomial\"] = sympy.binomial\n        safe_globals[\"isprime\"] = sympy.isprime\n        safe_globals[\"prime\"] = sympy.prime\n        safe_globals[\"primerange\"] = sympy.primerange\n        safe_globals[\"factorint\"] = sympy.factorint\n        safe_globals[\"divisors\"] = sympy.divisors\n        safe_globals[\"Matrix\"] = sympy.Matrix\n        safe_globals[\"floor\"] = sympy.floor\n        safe_globals[\"ceiling\"] = sympy.ceiling\n        safe_globals[\"Abs\"] = sympy.Abs\n        \n    if np is not None:\n        safe_globals[\"np\"] = np\n        safe_globals[\"numpy\"] = np\n    \n    stdout_capture = StringIO()\n    \n    try:\n        with redirect_stdout(stdout_capture):\n            exec(code, safe_globals)\n        return True, stdout_capture.getvalue()\n    except Exception as e:\n        return False, str(e)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_answer(text: str) -> Optional[int]:\n    \"\"\"Extract final answer from text with enhanced pattern matching\"\"\"\n    \n    # Method 1: Look for \\boxed{...} pattern (highest priority)\n    boxed_patterns = [\n        r'\\\\boxed\\{(\\d+)\\}',  # \\boxed{123}\n        r'\\\\boxed\\{([^}]+)\\}',  # \\boxed{anything}\n        r'boxed\\{(\\d+)\\}',  # boxed{123} without backslash\n    ]\n    \n    for pattern in boxed_patterns:\n        matches = re.findall(pattern, text)\n        if matches:\n            answer_str = matches[-1].strip()\n            # Extract numbers from the match\n            numbers = re.findall(r'-?\\d+', answer_str)\n            if numbers:\n                try:\n                    val = int(numbers[-1])\n                    return val % 100000  # Ensure 5 digits max\n                except:\n                    pass\n    \n    # Method 2: Look for explicit answer statements\n    answer_patterns = [\n        r'(?:the\\s+)?(?:final\\s+)?answer\\s+is\\s*[:\\s]*(\\d+)',\n        r'(?:the\\s+)?remainder\\s+is\\s*[:\\s]*(\\d+)',\n        r'(?:the\\s+)?result\\s+is\\s*[:\\s]*(\\d+)',\n        r'(?:therefore|thus|hence|so)[,\\s]+(?:the\\s+)?(?:answer\\s+is\\s+)?(\\d+)',\n        r'=\\s*(\\d+)\\s*(?:\\(mod\\s*\\d+\\))?$',\n        r'answer[:\\s]+(\\d+)',\n    ]\n    \n    for pattern in answer_patterns:\n        matches = re.findall(pattern, text.lower())\n        if matches:\n            try:\n                val = int(matches[-1])\n                return val % 100000\n            except:\n                pass\n    \n    # Method 3: Look for the last standalone number in the text\n    # Focus on the last portion of the response\n    last_section = text[-1000:]  # Last 1000 characters\n    \n    # Find numbers that look like final answers\n    final_patterns = [\n        r'(?:^|\\n)\\s*(\\d+)\\s*$',  # Number on its own line at the end\n        r'(?:is|=|:)\\s*(\\d+)\\s*\\.?\\s*$',  # Number after is/=/: at end\n    ]\n    \n    for pattern in final_patterns:\n        matches = re.findall(pattern, last_section.strip())\n        if matches:\n            try:\n                val = int(matches[-1])\n                if 0 <= val < 100000:  # Reasonable answer range\n                    return val\n            except:\n                pass\n    \n    # Method 4: Last resort - find any number in the last part\n    numbers = re.findall(r'\\b(\\d+)\\b', last_section)\n    if numbers:\n        # Filter out very small numbers that are likely not answers\n        candidates = [int(n) for n in numbers if len(n) >= 1]\n        if candidates:\n            return candidates[-1] % 100000\n    \n    return None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model using transformers - 7B fits easily without quantization\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\n\nprint(\"Loading model...\")\n\n# Verify model path exists\nmodel_path = Config.model_id\nif not os.path.exists(model_path):\n    print(f\"WARNING: Model path does not exist: {model_path}\")\n    print(\"Available inputs:\")\n    input_dir = \"/kaggle/input\"\n    if os.path.exists(input_dir):\n        for item in os.listdir(input_dir):\n            item_path = os.path.join(input_dir, item)\n            print(f\"  - {item_path}\")\n            if os.path.isdir(item_path):\n                for sub in os.listdir(item_path)[:5]:\n                    print(f\"      - {sub}\")\n    \n    # Try to find any available qwen model variant\n    qwen_dir = \"/kaggle/input/qwen2.5-math/transformers\"\n    if os.path.exists(qwen_dir):\n        print(f\"\\nAvailable Qwen variants in {qwen_dir}:\")\n        for variant in os.listdir(qwen_dir):\n            print(f\"  - {variant}\")\n    \n    raise FileNotFoundError(f\"Model not found at {model_path}\")\nelse:\n    print(f\"✓ Model path verified: {model_path}\")\n\n# Load tokenizer\nprint(\"\\nLoading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(\n    model_path, \n    trust_remote_code=True\n)\n\n# Load model - 7B in bfloat16 is only ~14GB, fits easily on H100 (80GB)\nprint(\"Loading model (7B fits easily without quantization)...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    low_cpu_mem_usage=True,\n)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"\\n✓ Model loaded successfully!\")\nprint(f\"  Path: {model_path}\")\nprint(f\"  Device: {model.device}\")\nprint(f\"  VRAM usage: ~14GB (plenty of room on H100)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_response(prompt: str, max_tokens: int = None) -> str:\n    \"\"\"Generate a single response from the model\"\"\"\n    if max_tokens is None:\n        max_tokens = Config.max_new_tokens\n        \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            temperature=Config.temperature,\n            top_p=Config.top_p,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    \n    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n    return response\n\ndef solve_with_feedback(problem: str) -> Optional[int]:\n    \"\"\"Solve a problem with code execution feedback loop\"\"\"\n    \n    # Initial prompt\n    prompt = f\"{SYSTEM_PROMPT}\\n\\nProblem: {problem}\\n\\nSolution:\"\n    response = generate_response(prompt)\n    \n    # Feedback loop - execute code and provide results back\n    if Config.enable_feedback_loop:\n        for turn in range(Config.max_feedback_turns):\n            code_blocks = extract_code_blocks(response)\n            \n            if not code_blocks:\n                break\n                \n            # Execute all code blocks\n            code_results = []\n            for i, code in enumerate(code_blocks[:Config.max_code_executions]):\n                success, output = execute_code_safely(code)\n                if success and output.strip():\n                    code_results.append(f\"Code block {i+1} output:\\n{output.strip()}\")\n                elif not success:\n                    code_results.append(f\"Code block {i+1} error:\\n{output}\")\n            \n            if not code_results:\n                break\n                \n            # Create feedback prompt\n            feedback = \"\\n\\n\".join(code_results)\n            feedback_prompt = f\"{prompt}\\n\\n{response}\\n\\n**Code Execution Results:**\\n{feedback}\\n\\nBased on these results, continue your solution and provide the final answer in \\\\boxed{{}}:\"\n            \n            # Get updated response\n            response = response + \"\\n\\n**Code Results:**\\n\" + feedback + \"\\n\\n\" + generate_response(feedback_prompt, max_tokens=1024)\n    \n    # Self-verification\n    if Config.enable_verification:\n        answer = extract_answer(response)\n        if answer is not None:\n            verify_prompt = f\"{prompt}\\n\\n{response}\\n\\n{VERIFY_PROMPT}\"\n            verification = generate_response(verify_prompt, max_tokens=512)\n            verified_answer = extract_answer(verification)\n            if verified_answer is not None:\n                return verified_answer\n    \n    return extract_answer(response)\n\ndef solve_problem(problem: str) -> int:\n    \"\"\"Solve a math problem using enhanced SC-TIR with feedback and verification\"\"\"\n    \n    answers = []\n    \n    for i in range(Config.num_samples):\n        try:\n            answer = solve_with_feedback(problem)\n            if answer is not None:\n                answers.append(answer)\n                \n            # Progress indicator every 8 samples\n            if (i + 1) % 8 == 0:\n                print(f\"    Progress: {i+1}/{Config.num_samples} samples\")\n                \n        except Exception as e:\n            print(f\"    Sample {i+1} error: {e}\")\n            continue\n    \n    # Enhanced majority voting with confidence\n    if answers:\n        counter = Counter(answers)\n        most_common = counter.most_common()\n        \n        # Print voting results\n        print(f\"  Votes: {dict(most_common[:5])}\")\n        \n        # Check confidence (if top answer has >50% votes, use it)\n        top_answer, top_count = most_common[0]\n        confidence = top_count / len(answers)\n        print(f\"  Confidence: {confidence:.1%} ({top_count}/{len(answers)})\")\n        \n        return top_answer\n    \n    print(\"  Warning: No valid answers found!\")\n    return 0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_path = '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv'\n",
    "test_df = pl.read_csv(test_path)\n",
    "print(f\"Loaded {len(test_df)} problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve all problems\n",
    "results = []\n",
    "\n",
    "for row in test_df.iter_rows(named=True):\n",
    "    problem_id = row['id']\n",
    "    problem = row['problem']\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Solving: {problem_id}\")\n",
    "    print(f\"Problem: {problem[:100]}...\")\n",
    "    \n",
    "    start = time.time()\n",
    "    answer = solve_problem(problem)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Answer: {answer} (took {elapsed:.1f}s)\")\n",
    "    \n",
    "    results.append({'id': problem_id, 'answer': answer})\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Completed all {len(results)} problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission\n",
    "submission_df = pl.DataFrame(results)\n",
    "submission_df.write_csv('/kaggle/working/submission.csv')\n",
    "print(\"Submission saved!\")\n",
    "print(submission_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}